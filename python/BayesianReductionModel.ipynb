{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pymc version is 5.10.4.\n"
     ]
    }
   ],
   "source": [
    "#%% \n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.constrained_layout.use':True})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import os\n",
    "import pytensor.tensor as pt\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import norm\n",
    "from xarray_einstats.stats import XrContinuousRV\n",
    "\n",
    "directory = os.fsencode(\"/Users/sydney/git/second-order-contacts/data/DataBimodalFit\") #Please put absolute path here, when reproducing the fits\n",
    "os.listdir(directory)\n",
    "\n",
    "print('The pymc version is {}.' .format(pm.__version__))\n",
    "#Comment: This notebook is based on/follows the structure of https://www.pymc.io/projects/examples/en/2021.11.0/mixture_models/gaussian_mixture_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10da0b850>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sydney/git/second-order-contacts/myenv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(directory):\n",
    "        \n",
    "        if os.fsdecode(file) != \".DS_Store\":\n",
    "        \n",
    "                filename = \"/Users/sydney/git/second-order-contacts/data/DataBimodalFit/\" + os.fsdecode(file) #Put absolute path here when reproducing the fits\n",
    "                file = os.fsdecode(file)\n",
    "                dataframe = pd.read_csv(\n",
    "                        filename, index_col = 0\n",
    "                )\n",
    "                data = dataframe[\"value\"].to_xarray()\n",
    "                lendata = len(dataframe.index)\n",
    "\n",
    "                #fixed_mean = dataframe['fixed_mean'].to_numpy()\n",
    "                \n",
    "                k = 1\n",
    "\n",
    "                with pm.Model(coords={\"cluster\": np.arange(k), \"obs_id\": np.arange(lendata)}) as model_1:\n",
    "                        sigma_1 = pm.Uniform(\"sigma_1\", lower=1, upper=10)\n",
    "                        #sigma_1 = pm.HalfCauchy(\"sigma_1\", beta = 10)\n",
    "                        #sigma_1 = pm.Gamma(\"sigma_1\", alpha = 10, beta = 1)\n",
    "                        points = pm.TruncatedNormal(\"obs\", mu=-50, sigma=sigma_1, lower = -100, observed=data, dims=\"obs_id\")\n",
    "                \n",
    "                pm.model_to_graphviz(model_1)\n",
    "                \n",
    "                with model_1:\n",
    "                        trace1 = pm.sample(2000, model=model_1, return_inferencedata=True,cores=4, chains=4,idata_kwargs={\"log_likelihood\": True})\n",
    "                        axes = az.plot_trace(trace1, var_names=[\"sigma_1\"]);\n",
    "                        \n",
    "                        xi = np.linspace(-100, 75, 500)\n",
    "                        post = trace1.posterior\n",
    "                        summary = az.summary(post)\n",
    "                        outputfilename = os.fsdecode(file) + \"posteriorunimodal.csv\"\n",
    "                        summary.to_csv(outputfilename)\n",
    "                        #First Component\n",
    "                        fixed_mean_1 = -100\n",
    "                        pdf_components_1 = XrContinuousRV(stats.truncnorm, loc=fixed_mean_1, scale=post[\"sigma_1\"], a = -100, b =100).pdf(xi)\n",
    "                        pdf_1 = pdf_components_1.sum() \n",
    "                        \n",
    "                        fig, ax = plt.subplots(4, 1, figsize=(7, 12), sharex=True, layout=\"constrained\")\n",
    "                        # empirical histogram\n",
    "                        ax[0].hist(data, bins = 35)\n",
    "                        ax[0].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        # pdf\n",
    "                        pdf_components_1.mean(dim=[\"chain\", \"draw\"]).plot.line(ax=ax[1])\n",
    "                        ax[1].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\\ndensity\")\n",
    "                        #Combination of histogram and pdf\n",
    "                        ax[2].hist(data, bins = 35)\n",
    "                        ax[2].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        pdf_comb = pdf_components_1.mean(dim=[\"chain\", \"draw\"])*5500\n",
    "                        pdf_comb.plot.line(ax=ax[2])\n",
    "                        ax[2].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency + Scaled Probability\\ndensity\")\n",
    "                        # plot group membership probabilities\n",
    "                        first_group = pdf_components_1.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        first_group.plot.line(ax=ax[3])\n",
    "                        ax[3].set(title=\"Group membership\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\");\n",
    "                        \n",
    "                        df = pd.DataFrame(first_group)\n",
    "                        file_name = os.fsdecode(file) + \"unimodalGroups.csv\"\n",
    "                        df.to_csv(file_name)\n",
    "                        \n",
    "                        fig_name = os.fsdecode(file) + \"unimodalOutput.png\"\n",
    "                        fig.savefig(fig_name, bbox_inches=\"tight\")\n",
    "                        \n",
    "                k = 2 #Fitting two distributions to the data\n",
    "\n",
    "                with pm.Model(coords={\"cluster\": np.arange(k), \"obs_id\": np.arange(lendata)}) as model_2:\n",
    "                        #cluster sizes\n",
    "                        weights = pm.Dirichlet(\"weights\", a=np.array([1,1]), dims=\"cluster\", shape=(2))\n",
    "                        #ensure all clusters have some points\n",
    "                        category = pm.Categorical(\"category\", p = weights, dims = \"obs_id\")\n",
    "                        \n",
    "                        sigma_1 = pm.Uniform(\"sigma_1\", lower=1, upper=7)\n",
    "                        sigma_2 = pm.Uniform(\"sigma_2\", lower=1, upper=7)\n",
    "                        \n",
    "                        truncnorm_dist = pm.TruncatedNormal.dist(mu = -100, sigma = sigma_1, lower = -100)\n",
    "                        norm_dist = pm.Normal.dist(mu = 0, sigma = sigma_2)\n",
    "                        \n",
    "                        components = [\n",
    "                        truncnorm_dist,\n",
    "                        norm_dist\n",
    "                        ]\n",
    "                        \n",
    "                        points = pm.Mixture(\"obs\", w=weights, comp_dists=components, observed=data, dims = \"obs_id\")\n",
    "                \n",
    "                pm.model_to_graphviz(model_2)\n",
    "                \n",
    "                with model_2:\n",
    "                        step1 = pm.Metropolis(vars=[weights, sigma_1, sigma_2])\n",
    "                        step2 = pm.CategoricalGibbsMetropolis(vars=[category])\n",
    "                        trace2 = pm.sample(2000, model=model_2, step=[step1, step2], return_inferencedata=True,cores=4, chains=4,idata_kwargs={\"log_likelihood\": True})\n",
    "                        axes = az.plot_trace(trace2, var_names=[\"weights\", \"sigma_1\", \"sigma_2\"]);\n",
    "                        \n",
    "                        xi = np.linspace(-100, 75, 500)\n",
    "                        post = trace2.posterior\n",
    "                        summary = az.summary(post)\n",
    "                        outputfilename = os.fsdecode(file) + \"posteriorbimodal.csv\"\n",
    "                        summary.to_csv(outputfilename)\n",
    "                        #First Component\n",
    "                        fixed_mean_1 = -100\n",
    "                        pdf_components_1 = XrContinuousRV(stats.truncnorm, loc=fixed_mean_1, scale=post[\"sigma_1\"], a = -100, b =100).pdf(xi) * post[\"weights\"].sel(cluster=0)\n",
    "                        pdf_1 = pdf_components_1.sum() \n",
    "                        #Second Component\n",
    "                        fixed_mean_2 = 0\n",
    "                        pdf_components_2 = XrContinuousRV(norm, loc=fixed_mean_2, scale=post[\"sigma_2\"]).pdf(xi) * post[\"weights\"].sel(cluster=1)\n",
    "                        pdf_2 = pdf_components_2.sum()\n",
    "\n",
    "                        fig, ax = plt.subplots(4, 1, figsize=(7, 12), sharex=True, layout=\"constrained\")\n",
    "                        # empirical histogram\n",
    "                        ax[0].hist(data, bins = 35)\n",
    "                        ax[0].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        # pdf\n",
    "                        pdf_combination = pdf_components_1.mean(dim=[\"chain\", \"draw\"]) + pdf_components_2.mean(dim=[\"chain\", \"draw\"])\n",
    "                        pdf_combination.plot.line(ax=ax[1])\n",
    "                        ax[1].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\\ndensity\")\n",
    "                        #Combination of histogram and pdf\n",
    "                        ax[2].hist(data, bins = 35)\n",
    "                        ax[2].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        pdf_comb = pdf_combination*2000\n",
    "                        pdf_comb.plot.line(ax=ax[2])\n",
    "                        ax[2].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency + Scaled Probability\\ndensity\")\n",
    "                        # plot group membership probabilities\n",
    "                        first_group = pdf_components_1.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        first_group.plot.line(ax=ax[3])\n",
    "                        second_group = pdf_components_2.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        second_group.plot.line(ax=ax[3])\n",
    "                        ax[3].set(title=\"Group membership\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\");\n",
    "                        \n",
    "                        df = pd.DataFrame({\n",
    "                                \"first_group\": first_group,\n",
    "                                \"second_group\": second_group}\n",
    "                                )\n",
    "                        file_name = os.fsdecode(file) + \"bimodalGroups.csv\"\n",
    "                        df.to_csv(file_name)\n",
    "                        \n",
    "                        fig_name = os.fsdecode(file) + \"bimodalOutput.png\"\n",
    "                        fig.savefig(fig_name, bbox_inches=\"tight\")\n",
    "                 \n",
    "                k = 3 # Fitting 3 distributions to the data\n",
    "\n",
    "                with pm.Model(coords={\"cluster\": np.arange(k), \"obs_id\": np.arange(lendata)}) as model_3:\n",
    "                        #cluster sizes\n",
    "                        weights = pm.Dirichlet(\"weights\", a=np.array([1,1,1]), dims=(\"cluster\"), shape=(3))\n",
    "   \n",
    "                        sigma_1 = pm.Uniform(\"sigma_1\", lower=1, upper=7)\n",
    "                        sigma_2 = pm.Uniform(\"sigma_2\", lower=1, upper=15)\n",
    "                        sigma_3 = pm.Uniform(\"sigma_3\", lower=1, upper=7)\n",
    "                        \n",
    "                        truncnorm_dist = pm.TruncatedNormal.dist(mu = -100, sigma = sigma_1, lower = -100)\n",
    "                        norm_dist_1 = pm.Normal.dist(mu = -50, sigma = sigma_2)\n",
    "                        norm_dist_2 = pm.Normal.dist(mu = 0, sigma = sigma_3)\n",
    "                        \n",
    "                        components = [\n",
    "                        truncnorm_dist,\n",
    "                        norm_dist_1,\n",
    "                        norm_dist_2\n",
    "                        ]\n",
    "                        \n",
    "                        points = pm.Mixture(\"obs\", w=weights, comp_dists=components, observed=data, dims = \"obs_id\")\n",
    "                pm.model_to_graphviz(model_3)\n",
    "\n",
    "                with model_3:\n",
    "                        step1 = pm.Metropolis(vars=[weights, sigma_1, sigma_2, sigma_3])\n",
    "                        #step2 = pm.CategoricalGibbsMetropolis(vars=[category])\n",
    "                        trace3 = pm.sample(2000, model=model_3, step=[step1], return_inferencedata=True,cores=4, chains=4,idata_kwargs={\"log_likelihood\": True})\n",
    "                        axes = az.plot_trace(trace3, var_names=[\"weights\", \"sigma_1\", \"sigma_2\", \"sigma_3\"]);\n",
    "                        \n",
    "                        xi = np.linspace(-100, 75, 500)\n",
    "                        post = trace3.posterior\n",
    "                        summary = az.summary(post)\n",
    "                        outputfilename = os.fsdecode(file) + \"posteriortrimodal.csv\"\n",
    "                        summary.to_csv(outputfilename)\n",
    "                        #First Component\n",
    "                        fixed_mean_1 = -100\n",
    "                        pdf_components_1 = XrContinuousRV(stats.truncnorm, loc=fixed_mean_1, scale=post[\"sigma_1\"], a = -100, b =100).pdf(xi) * post[\"weights\"].sel(cluster=0)\n",
    "                        #Second Component\n",
    "                        fixed_mean_2 = -50\n",
    "                        pdf_components_2 = XrContinuousRV(norm, loc=fixed_mean_2, scale=post[\"sigma_2\"]).pdf(xi) * post[\"weights\"].sel(cluster=1)\n",
    "                        #Third Component\n",
    "                        fixed_mean_3 = 0\n",
    "                        pdf_components_3 = XrContinuousRV(norm, loc=fixed_mean_3, scale=post[\"sigma_3\"]).pdf(xi) * post[\"weights\"].sel(cluster=2)\n",
    "\n",
    "                        fig, ax = plt.subplots(4, 1, figsize=(7, 12), sharex=True, layout=\"constrained\")\n",
    "                        # empirical histogram\n",
    "                        ax[0].hist(data, bins = 35)\n",
    "                        ax[0].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        # pdf\n",
    "                        pdf_combination = pdf_components_1.mean(dim=[\"chain\", \"draw\"]) + pdf_components_2.mean(dim=[\"chain\", \"draw\"]) + pdf_components_3.mean(dim=[\"chain\", \"draw\"])\n",
    "                        pdf_combination.plot.line(ax=ax[1])\n",
    "                        ax[1].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\\ndensity\")\n",
    "                        #Combination of histogram and pdf\n",
    "                        ax[2].hist(data, bins = 35)\n",
    "                        ax[2].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        pdf_comb = pdf_combination*2000\n",
    "                        pdf_comb.plot.line(ax=ax[2])\n",
    "                        ax[2].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency + Scaled Probability\\ndensity\")\n",
    "                        # plot group membership probabilities\n",
    "                        first_group = pdf_components_1.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        first_group.plot.line(ax=ax[3])\n",
    "                        second_group = pdf_components_2.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        second_group.plot.line(ax=ax[3])\n",
    "                        third_group = pdf_components_3.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        third_group.plot.line(ax=ax[3])\n",
    "                        ax[3].set(title=\"Group membership\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\");\n",
    "                        \n",
    "                        fig_name = os.fsdecode(file) + \"trimodalOutput.png\"\n",
    "                        fig.savefig(fig_name, bbox_inches=\"tight\")\n",
    "                        \n",
    "                        df = pd.DataFrame({\n",
    "                                \"first_group\": first_group,\n",
    "                                \"second_group\": second_group,\n",
    "                                \"third_group\": third_group}\n",
    "                                )\n",
    "                        file_name = os.fsdecode(file) + \"trimodalGroups.csv\"\n",
    "                        df.to_csv(file_name)\n",
    "                        \n",
    "\n",
    "                k = 4 #Fitting 4 distributions to the data\n",
    "\n",
    "                with pm.Model(coords={\"cluster\": np.arange(k), \"obs_id\": np.arange(lendata)}) as model_4:\n",
    "                        #cluster sizes\n",
    "                        weights = pm.Dirichlet(\"weights\", a=np.array([1,1,1,1]), dims=(\"cluster\"), shape=(4))\n",
    "                        \n",
    "                        sigma_1 = pm.Uniform(\"sigma_1\", lower=1, upper=7)\n",
    "                        sigma_2 = pm.Uniform(\"sigma_2\", lower=1, upper=15)\n",
    "                        sigma_3 = pm.Uniform(\"sigma_3\", lower=1, upper=7)\n",
    "                        sigma_4 = pm.Uniform(\"sigma_4\", lower=1, upper=15)\n",
    "                        \n",
    "                        truncnorm_dist = pm.TruncatedNormal.dist(mu = -100, sigma = sigma_1, lower = -100)\n",
    "                        norm_dist_1 = pm.Normal.dist(mu = -50, sigma = sigma_2)\n",
    "                        norm_dist_2 = pm.Normal.dist(mu = 0, sigma = sigma_3)\n",
    "                        norm_dist_3 = pm.Normal.dist(mu = 50, sigma = sigma_4)\n",
    "                        \n",
    "                        components = [\n",
    "                        truncnorm_dist,\n",
    "                        norm_dist_1,\n",
    "                        norm_dist_2,\n",
    "                        norm_dist_3\n",
    "                        ]\n",
    "                        \n",
    "                        points = pm.Mixture(\"obs\", w=weights, comp_dists=components, observed=data, dims = \"obs_id\")\n",
    "\n",
    "                with model_4:\n",
    "                        step1 = pm.Metropolis(vars=[weights, sigma_1, sigma_2, sigma_3, sigma_4])\n",
    "                        trace4 = pm.sample(2000, model=model_4, step=[step1], return_inferencedata=True,cores=4, chains=4,idata_kwargs={\"log_likelihood\": True})\n",
    "                        axes = az.plot_trace(trace4, var_names=[\"weights\", \"sigma_1\", \"sigma_2\", \"sigma_3\", \"sigma_4\"]);\n",
    "                        \n",
    "                        xi = np.linspace(-100, 75, 500)\n",
    "                        post = trace4.posterior\n",
    "                        summary = az.summary(post)\n",
    "                        outputfilename = os.fsdecode(file) + \"posteriorquatromodal.csv\"\n",
    "                        summary.to_csv(outputfilename)\n",
    "                        #First Component\n",
    "                        fixed_mean_1 = -100\n",
    "                        pdf_components_1 = XrContinuousRV(stats.truncnorm, loc=fixed_mean_1, scale=post[\"sigma_1\"], a = -100, b =100).pdf(xi) * post[\"weights\"].sel(cluster=0)\n",
    "                        #Second Component\n",
    "                        fixed_mean_2 = -50\n",
    "                        pdf_components_2 = XrContinuousRV(norm, loc=fixed_mean_2, scale=post[\"sigma_2\"]).pdf(xi) * post[\"weights\"].sel(cluster=1)\n",
    "                        #Third Component\n",
    "                        fixed_mean_3 = 0\n",
    "                        pdf_components_3 = XrContinuousRV(norm, loc=fixed_mean_3, scale=post[\"sigma_3\"]).pdf(xi) * post[\"weights\"].sel(cluster=2)\n",
    "                        #Fourth Component\n",
    "                        fixed_mean_4 = 50\n",
    "                        pdf_components_4 = XrContinuousRV(norm, loc=fixed_mean_4, scale=post[\"sigma_4\"]).pdf(xi) * post[\"weights\"].sel(cluster=3)\n",
    "\n",
    "\n",
    "                        fig, ax = plt.subplots(4, 1, figsize=(7, 12), sharex=True, layout=\"constrained\")\n",
    "                        # empirical histogram\n",
    "                        ax[0].hist(data, bins = 35\n",
    "                                   )\n",
    "                        ax[0].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        # pdf\n",
    "                        pdf_combination = pdf_components_1.mean(dim=[\"chain\", \"draw\"]) + pdf_components_2.mean(dim=[\"chain\", \"draw\"]) + pdf_components_3.mean(dim=[\"chain\", \"draw\"]) + pdf_components_4.mean(dim=[\"chain\", \"draw\"])\n",
    "                        pdf_combination.plot.line(ax=ax[1])\n",
    "                        ax[1].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\\ndensity\")\n",
    "                        #Combination of histogram and pdf\n",
    "                        ax[2].hist(data, bins = 35)\n",
    "                        ax[2].set(title=\"Data\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency\")\n",
    "                        pdf_comb = pdf_combination*2000\n",
    "                        pdf_comb.plot.line(ax=ax[2])\n",
    "                        ax[2].set(title=\"PDF\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Frequency + Scaled Probability\\ndensity\")\n",
    "                        # plot group membership probabilities\n",
    "                        first_group = pdf_components_1.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"])+pdf_components_4.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        first_group.plot.line(ax=ax[3])\n",
    "                        second_group = pdf_components_2.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"])+pdf_components_4.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        second_group.plot.line(ax=ax[3])\n",
    "                        third_group = pdf_components_3.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"])+pdf_components_4.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        third_group.plot.line(ax=ax[3])\n",
    "                        fourth_group = pdf_components_4.mean(dim=[\"chain\", \"draw\"])/(pdf_components_1.mean(dim=[\"chain\", \"draw\"])+pdf_components_2.mean(dim=[\"chain\", \"draw\"])+pdf_components_3.mean(dim=[\"chain\", \"draw\"])+pdf_components_4.mean(dim=[\"chain\", \"draw\"]))\n",
    "                        fourth_group.plot.line(ax=ax[3])\n",
    "                        ax[3].set(title=\"Group membership\", xlabel=\"Change of No. of Contacts (in percent)\", ylabel=\"Probability\");\n",
    "                        \n",
    "                        df = pd.DataFrame({\n",
    "                                \"first_group\": first_group,\n",
    "                                \"second_group\": second_group,\n",
    "                                \"third_group\": third_group,\n",
    "                                \"fourth_group\":fourth_group}\n",
    "                                )\n",
    "                        file_name = os.fsdecode(file) + \"quatromodalGroups.csv\"\n",
    "                        df.to_csv(file_name)\n",
    "                        \n",
    "                        fig_name = os.fsdecode(file) + \"quatromodalOutput.png\"\n",
    "                        fig.savefig(fig_name, bbox_inches=\"tight\")\n",
    "                \n",
    "                # Comparison of models via leave-one-out cross validation \n",
    "                df_comp_loo = az.compare({\"one_dist\":trace1,  \"two_dist\":trace2, \"three_dist\":trace3,  \"four_dist\":trace4})\n",
    "                outputfilename =  os.fsdecode(file) + \"loo_comparison.csv\"\n",
    "                df_comp_loo.to_csv(outputfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_group</th>\n",
       "      <th>second_group</th>\n",
       "      <th>third_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994429</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.993993</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992688</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.989960</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.984468</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     first_group  second_group  third_group\n",
       "0       0.994429      0.005571          0.0\n",
       "1       0.993993      0.006007          0.0\n",
       "2       0.992688      0.007312          0.0\n",
       "3       0.989960      0.010040          0.0\n",
       "4       0.984468      0.015532          0.0\n",
       "..           ...           ...          ...\n",
       "495     0.000000      1.000000          0.0\n",
       "496     0.000000      1.000000          0.0\n",
       "497     0.000000      1.000000          0.0\n",
       "498     0.000000      1.000000          0.0\n",
       "499     0.000000      1.000000          0.0\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
